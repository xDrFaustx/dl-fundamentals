type: order_cards
visible: false
header: Maximal Likelihood for Normal Distribution
text: |
  Order cards to get derivation of loglikelihood of
  normal distribution.
shuffle: true
score: 1
quiz:
  variants:
    - text: >
        After simplification, it becomes:


        $$\sigma^2 = \frac{1}{k} \sum_k (x_k - \mu)^2$$


        I.e., optimal parameter \sigma^2 for Normal Distribution is equal to
        the 

        mean of squared deviations.
    - text: >
        Consider a set of samples 


        $x_1, x_2, \dots, x_n$


        that are supposedly drawn from Normal Distribution:


        $p(x) = \frac{1}{\sqrt{2 \pi} \sigma} \exp(- \frac{(x -
        \mu)^2}{2\sigma^2} )$


        where $\sigma$ is standard deviation of $x$

        and $\mu$ is mean


        Thus the task is to find parameters $\sigma$ and $\mu$ that

        the best fit for the set of samples
    - text: >
        That after simplification turns into


        $$\mu = \frac{\sum_k x_k}{k}$$


        I.e., optimal parameter $\mu$ for Normal Distribution is the mean of the
        observed samples
    - text: |
        Let us find probability of the set of samples assuming that
        every sample is independent on the others:

        $P(x_1, x_2, \dots, x_N) = \prod_k p(x_k)$
    - text: |
        It is equal to 

        $$\frac{k}{\sigma} - \sum_k \frac{(x_k - \mu)^2}{\sigma^3} = 0$$
    - text: >
        Turning maximization problem into standard minimization problem,

        we get the loss function:


        $$L = k \log \sigma + \sum_k \frac{(x_k - \mu)^2}{2 \sigma^2}$$


        that is surprisingly similar to the Squared Error (up to a scaling
        factor and an additional term)
    - text: |
        Consider now 

        $$\frac{\partial L}{\partial \sigma} = 0$$
    - text: >
        Setting this derivative to zero (with accordance with extremum
        condition), we get equation:


        $$\frac{1}{2 \sigma^2} \sum_k x_k - \frac{k \mu}{2 \sigma^2} = 0$$
    - text: >
        The best parameters will be such $\Sigma$ and $\mu$ that are the 

        solution of the following optization problem:


        $$\sigma, \mu = \underset{\sigma, \mu}{\textrm{argmin}} \left[k \log
        \sigma + \sum_k \frac{(x_k - \mu)^2}{2 \sigma^2} \right]$$
    - text: |
        The first term is a constant and cannot be optimized, while the
        second and third ones can be maximized by tuning $\mu$ and $\Sigma$
    - text: >
        Now one can find optimal value for $\mu$ by taking a derivative


        $$\frac{\partial L}{\partial \mu} = \frac{1}{\sigma^2} \sum_k (x_k -
        \mu) = \frac{1}{2 \sigma^2} \sum_k x_k - \frac{k \mu}{2 \sigma^2}$$
    - text: >
        Take logarithm of the probability of the observed set to get rid

        of summation:


        $$\log P(x_1, x_2, \dots, x_N) = $$

        $$= \sum_k \log p(x_k) =$$

        $$= - k \frac{1}{2} \log (2\pi) - k \log \sigma - \sum_k \frac{(x_k -
        \mu)^2}{2 \sigma^2}$$
answer:
  - c4ca4238a0b923820dcc509a6f75849b
  - eccbc87e4b5ce2fe28308fd9f2a7baf3
  - 6512bd43d9caa6e02c990b0a82652dca
  - 45c48cce2e2d7fbdea1afc51c7c6ad26
  - e4da3b7fbbce2345d7772b0674a318d5
  - c9f0f895fb98ab9159f51fd0297e236d
  - d3d9446802a44259755d38e6d163e820
  - 8f14e45fceea167a5a36dedd4bea2543
  - c81e728d9d4c2f636f067f89cc14862c
  - 1679091c5a880faf6fb5e6087eb1b2dc
  - a87ff679a2f3e71d9181a67b7542122c
  - cfcd208495d565ef66e7dff9f98764da
